{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "resultat=pd.read_csv('C:\\\\Users\\\\Admin\\\\Documents\\\\projet_memoire\\\\PredictSecBugs\\\\aprrentissage\\\\result_total_bon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultat['buggy'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultat[resultat['buggy']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Exemple d'un DataFrame\n",
    "bugged_files = pd.read_csv('C:\\\\Users\\\\Admin\\\\Documents\\\\projet_memoire\\\\PredictSecBugs\\\\aprrentissage\\\\result_total_bon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugged_files.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "distribution = bugged_files.groupby(['Ecosystem', 'extension', 'Source']).size().unstack()\n",
    "\n",
    "\n",
    "distribution.plot(kind='bar', stacked=True)\n",
    "plt.title('Distribution des Fichiers Avant et Après Correction par Ecosystem et Extension')\n",
    "plt.xlabel('Ecosystem et Extension')\n",
    "plt.ylabel('Nombre de Fichiers')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Ecosystem', hue='Source', data=bugged_files)\n",
    "plt.title('Distribution des Fichiers Avant et Après Correction par Ecosystem')\n",
    "plt.xlabel('Ecosystem')\n",
    "plt.ylabel('Nombre de Fichiers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='extension', hue='Source', data=bugged_files)\n",
    "plt.title('Distribution des Fichiers Avant et Après Correction par Extension')\n",
    "plt.xlabel('Extension')\n",
    "plt.ylabel('Nombre de Fichiers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table = pd.crosstab([bugged_files['Ecosystem'], bugged_files['extension']], bugged_files['Source'])\n",
    "\n",
    "sns.heatmap(contingency_table, annot=True, cmap=\"YlGnBu\")\n",
    "plt.title('Heatmap des Fichiers Avant et Après Correction par Ecosystem et Extension')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bugged_files['commit_date'] = pd.to_datetime(bugged_files['commit_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugged_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugged_files[bugged_files['buggy']==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "# Function to load and preprocess the dataset\n",
    "def load_and_preprocess_data(file_path, selected_columns, target_column):\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = data.sort_values(by='commit_date')  # Sort by commit date\n",
    "\n",
    "    # Select features and target\n",
    "    X = data[selected_columns]\n",
    "    y = data[target_column]\n",
    "\n",
    "    # Convert 'commit_date' to numerical features\n",
    "    X = X.copy()\n",
    "    X.loc[:, 'commit_date'] = pd.to_datetime(X['commit_date']).astype(int) / 10**9\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Function to create the preprocessor\n",
    "def create_preprocessor(X):\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "            ]), categorical_cols),\n",
    "            ('num', Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='mean')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), numerical_cols)\n",
    "        ]\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "# Function to apply SMOTE\n",
    "def apply_smote(X, y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Main function to train and evaluate the model\n",
    "def train_and_evaluate_model(file_path, selected_columns, target_column, model: BaseEstimator):\n",
    "    # Load and preprocess data\n",
    "    X, y = load_and_preprocess_data(file_path, selected_columns, target_column)\n",
    "\n",
    "    # Create preprocessor\n",
    "    preprocessor = create_preprocessor(X)\n",
    "\n",
    "    # Preprocess data\n",
    "    X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "    # Apply SMOTE to oversample the minority class\n",
    "    X_resampled, y_resampled = apply_smote(X_preprocessed, y)\n",
    "\n",
    "    # Initialize TimeSeriesSplit for cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    scores = cross_val_score(model, X_resampled, y_resampled, cv=tscv, scoring='accuracy')\n",
    "    print(\"Cross-Validation Scores:\", scores)\n",
    "    print(\"Mean Accuracy:\", np.mean(scores))\n",
    "\n",
    "    # Train-test split for final evaluation\n",
    "    train_size = int(0.7 * len(X))\n",
    "    X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "    y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "    # Preprocess training and test sets\n",
    "    X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "    X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Apply SMOTE to training data\n",
    "    X_train_resampled, y_train_resampled = apply_smote(X_train_preprocessed, y_train)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_preprocessed)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Final Test Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # File path and columns\n",
    "    file_path = 'C:\\\\Users\\\\Utilisateur\\\\Desktop\\\\fichier_paquage\\\\1_A_finale\\\\result_total_bon.csv'\n",
    "    selected_columns = ['commit_date', 'extension', 'AvgLineCode', 'CountDeclClass', 'RatioCommentToCode',\n",
    "                        'CountStmtExe', 'AvgCyclomaticStrict', 'CountLine', 'SumCyclomatic',\n",
    "                        'AvgCyclomatic', 'SumEssential', 'MaxCyclomatic', 'AvgLineComment',\n",
    "                        'AvgCyclomaticModified', 'AvgEssential', 'SumCyclomaticModified',\n",
    "                        'CountLineComment', 'CountLineCode', 'MaxCyclomaticModified',\n",
    "                        'CountLineBlank', 'CountStmtDecl', 'AvgLine', 'MaxEssential',\n",
    "                        'CountDeclFunction', 'MaxNesting', 'AvgLineBlank',\n",
    "                        'SumCyclomaticStrict', 'CountStmt', 'Ecosystem']\n",
    "    target_column = 'buggy'\n",
    "\n",
    "    # Choose your model\n",
    "    chosen_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    train_and_evaluate_model(file_path, selected_columns, target_column, chosen_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset and sort it by the commit date\n",
    "data_path = 'C:\\\\Users\\\\Utilisateur\\\\Desktop\\\\fichier_paquage\\\\1_A_finale\\\\result_total_bon.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data = data.sort_values(by='commit_date')\n",
    "\n",
    "# Specify the columns to be used for training\n",
    "selected_columns = ['commit_date', 'extension', 'AvgLineCode', 'CountDeclClass', 'RatioCommentToCode',\n",
    "                    'CountStmtExe', 'AvgCyclomaticStrict', 'CountLine', 'SumCyclomatic', 'AvgCyclomatic',\n",
    "                    'SumEssential', 'MaxCyclomatic', 'AvgLineComment', 'AvgCyclomaticModified', 'AvgEssential',\n",
    "                    'SumCyclomaticModified', 'CountLineComment', 'CountLineCode', 'MaxCyclomaticModified',\n",
    "                    'CountLineBlank', 'CountStmtDecl', 'AvgLine', 'MaxEssential', 'CountDeclFunction',\n",
    "                    'MaxNesting', 'AvgLineBlank', 'SumCyclomaticStrict', 'CountStmt', 'Ecosystem']\n",
    "target_column = 'buggy'\n",
    "\n",
    "# Filter the DataFrame to keep only the selected columns\n",
    "X = data[selected_columns]\n",
    "y = data[target_column]\n",
    "\n",
    "# Convert 'commit_date' to numerical features (Unix timestamp)\n",
    "X = X.copy()\n",
    "X['commit_date'] = pd.to_datetime(X['commit_date']).view(np.int64) / 10**9\n",
    "\n",
    "# Split the data into training and test sets chronologically\n",
    "train_size = int(0.7 * len(X))\n",
    "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "# Identify categorical columns for encoding\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Create a preprocessor for handling missing values and encoding features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_cols),\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())  # Normalize numerical features\n",
    "        ]), X_train.select_dtypes(include=['float64', 'int64']).columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess the training data\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Apply SMOTE to handle class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_preprocessed, y_train)\n",
    "\n",
    "# Train a Random Forest model (can be replaced with any classifier)\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Preprocess the test data\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test_preprocessed)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Retrieve feature names after preprocessing\n",
    "encoded_categorical_names = preprocessor.named_transformers_['cat'].named_steps['encoder'].get_feature_names_out(categorical_cols)\n",
    "numerical_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "feature_names = list(encoded_categorical_names) + list(numerical_cols)\n",
    "\n",
    "# Calculate feature importance\n",
    "feature_importances = classifier.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display feature importance\n",
    "importances_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "\n",
    "# Sort features by importance\n",
    "importances_df = importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the feature importance DataFrame\n",
    "print(\"Feature Importances:\\n\", importances_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
